{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a9b6ec",
   "metadata": {},
   "source": [
    "# seq2seqCalculator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aae1df",
   "metadata": {},
   "source": [
    "## Import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8fca7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4444551",
   "metadata": {},
   "source": [
    "## Data\n",
    "One benefit of this task is that you don't need to download any data. You can generate it on your own! The input consists of the following: \"number\"+\"sign\"+\"number\", which then equals \"number\". The pluses stand for concatenation in this case. \n",
    "The seq2seq model doesn't care what symbols we use, we could use \"+-*/\" as symbols. \n",
    "<!-- We use a one-hot encoding for each symbol accroding the assigned number. For numbers this is straight forward and we end up with something like {0:0, 1:1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:7, 8: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "83534e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_equations(operators, dataset_size, min_value=0, max_value=10):\n",
    "    '''Generate pairs of equations and solutions to them.\n",
    "        \n",
    "        Each equation has a form of two integers with an operator in between. \n",
    "        Each solution is an integer with the result of the operation.\n",
    "        \n",
    "        \n",
    "        operators: list of strings, allowed operators. \n",
    "        dataset_size: an integer, number of equations to be generated.\n",
    "        min_value: an integer, min value of each operand.\n",
    "        max_value: an integer, max value of each operand.\n",
    "    \n",
    "        result: a list of tuples of strings (equation, solution).\n",
    "    '''\n",
    "    samples = []\n",
    "    number_of_operators = len(operators)\n",
    "    \n",
    "    for _ in range(dataset_size):\n",
    "        equation = (\n",
    "            str(randint(min_value, max_value)) + \n",
    "            operators[randint(0, number_of_operators-1)] +\n",
    "            str(randint(min_value, max_value))\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            solution = str(int(eval(equation)))\n",
    "        except ZeroDivisionError as e: ## handle x/0 state\n",
    "            equation = equation.replace(\"/\", \"*\")\n",
    "            solution = str(int(eval(equation)))\n",
    "\n",
    "        samples.append((equation, solution))\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c8a7229d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1*5', '5')]\n"
     ]
    }
   ],
   "source": [
    "## Test generate_equations\n",
    "test = generate_equations(\"+-/*\",1)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e747f7c1",
   "metadata": {},
   "source": [
    "## Perpare data for the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbfe646",
   "metadata": {},
   "source": [
    "The maximum input length is “length_nr + 1 + length_nr” which is ```x_max_length``` in our case (for example 7). We would like to also have calculations that are shorter. This is entirely possible, however a seq2seq requires [check] inputs of the same length. So when a calculation is smaller then length ```x_max_length``` we fill it up from the left with spaces, e.g. “____2+2”.\n",
    "Also we need to padd the solutions to ```y_max_length```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8f7c6d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_to_max(equations, x_max_length, y_max_length):\n",
    "    '''Padd the equations to max lengths.\n",
    "    \n",
    "    equations: a list of tuples of strings (equation, solution)\n",
    "    x_max_length: max_len of equation samples.\n",
    "    y_max_length: max_len of solution samples.\n",
    "    \n",
    "    return:  a list of tuples of strings (equation, solution) that padded to max length.\n",
    "    '''\n",
    "    samples = []\n",
    "    for equation, solution in equations:\n",
    "        samples.append((f'{equation:>{x_max_length}}', f'{solution:>{y_max_length}}'))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a0b03aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ids(equations, word2id):\n",
    "    '''Convert equations to ids. \n",
    "    '''\n",
    "    samples = []\n",
    "    for equation, solution in equations:\n",
    "        e = [word2id[c] for c in equation]\n",
    "        s = [word2id[c] for c in solution]\n",
    "        samples.append((e, s))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5bf18a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(equations, labels):\n",
    "    '''Create one-hot encoder for equations.\n",
    "    '''\n",
    "    label_binarizer = LabelBinarizer() \n",
    "    label_binarizer.fit(labels)\n",
    "    x, y = [], []\n",
    "    for equation, solution in equations:\n",
    "        x.append(label_binarizer.transform(equation))\n",
    "        y.append(label_binarizer.transform(solution))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45752b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert encoding\n",
    "def invert(seq, id2word):\n",
    "    strings = list()\n",
    "    for pattern in seq:\n",
    "        string = id2word[np.argmax(pattern)]\n",
    "        strings.append(string)\n",
    "    return ''.join(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0ec03ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples, allowed_operators, alphabet, word2id, id2word, x_max_length=7, y_max_length=4, min_value=0, max_value=100):\n",
    "    ## generate pairs\n",
    "    data = generate_equations(allowed_operators, n_samples, min_value=min_value, max_value=max_value)    \n",
    "\n",
    "    ## padding to max\n",
    "    data = padding_to_max(data, x_max_length, y_max_length)\n",
    "    \n",
    "    ## string to indexs\n",
    "    data = to_ids(data, word2id)\n",
    "    # one hot encoding \n",
    "    x, y = one_hot(data, list(id2word.keys()))\n",
    "\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    \n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0ced0b",
   "metadata": {},
   "source": [
    "### Setup variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a932a900",
   "metadata": {},
   "source": [
    "In this section we should set the number of samples, allowed opeatots and alphabet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c9162281",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "\n",
    "allowed_operators = '+'\n",
    "n_samples = 1000\n",
    "alphabet = '0123456789' + allowed_operators + ' ' ## space for padding\n",
    "\n",
    "word2id = {symbol:i for i, symbol in enumerate(alphabet)}\n",
    "id2word = {i:symbol for symbol, i in word2id.items()}\n",
    "\n",
    "x_max_length, y_max_length = 7, 4\n",
    "min_value, max_value = 0, 100\n",
    "\n",
    "x, y = generate_data(n_samples, allowed_operators, alphabet, word2id, id2word, x_max_length=x_max_length, y_max_length=y_max_length, \n",
    "                    min_value=min_value, max_value=max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9cbc69",
   "metadata": {},
   "source": [
    "Test the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ad51499d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X (1000, 7, 12)\n",
      "shape of y (1000, 4, 12)\n",
      "X[0]:\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0]]\n",
      "y[0]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0]]\n",
      "invert X[0]   17+32\n",
      "invert y[0]   49\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of X\", x.shape)\n",
    "print(\"shape of y\", y.shape)\n",
    "print(\"X[0]:\")\n",
    "print(x[0])\n",
    "print(\"y[0]\")\n",
    "print(y[0])\n",
    "print(\"invert X[0]\", invert(x[0], id2word) )\n",
    "print(\"invert y[0]\", invert(y[0], id2word) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c861b6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3f144dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LSTM configuration\n",
    "n_batch = 10\n",
    "n_epoch = 50\n",
    "n_chars = len(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "01f20fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LSTM\n",
    "def build_model(x_max_length, y_max_length, n_chars):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(x_max_length, n_chars)))\n",
    "    model.add(RepeatVector(y_max_length))\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(n_chars, activation='softmax')))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e50d3dc",
   "metadata": {},
   "source": [
    "### Configure checkpoints\n",
    "\n",
    "Use a ```tf.keras.callbacks.ModelCheckpoint``` to ensure that checkpoints are saved during training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "649574d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './models/training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a509ffaf",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4603584d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_54 (LSTM)              (None, 100)               45200     \n",
      "                                                                 \n",
      " repeat_vector_27 (RepeatVec  (None, 4, 100)           0         \n",
      " tor)                                                            \n",
      "                                                                 \n",
      " lstm_55 (LSTM)              (None, 4, 50)             30200     \n",
      "                                                                 \n",
      " time_distributed_27 (TimeDi  (None, 4, 12)            612       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 76,012\n",
      "Trainable params: 76,012\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# train LSTM\n",
    "model = build_model(x_max_length, y_max_length, n_chars)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0ec85fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100/100 [==============================] - 8s 11ms/step - loss: 0.0195 - accuracy: 0.8112\n",
      "1\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0192 - accuracy: 0.8223\n",
      "2\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0189 - accuracy: 0.8177\n",
      "3\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0198 - accuracy: 0.8092\n",
      "4\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0201 - accuracy: 0.8095\n",
      "5\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0194 - accuracy: 0.8160\n",
      "6\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0191 - accuracy: 0.8167\n",
      "7\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0180 - accuracy: 0.8345\n",
      "8\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0174 - accuracy: 0.8420\n",
      "9\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0185 - accuracy: 0.8332\n",
      "10\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0182 - accuracy: 0.8353\n",
      "11\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0178 - accuracy: 0.8450\n",
      "12\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0165 - accuracy: 0.8602\n",
      "13\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0161 - accuracy: 0.8633\n",
      "14\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0170 - accuracy: 0.8587\n",
      "15\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0161 - accuracy: 0.8700\n",
      "16\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0160 - accuracy: 0.8717\n",
      "17\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0165 - accuracy: 0.8650\n",
      "18\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0155 - accuracy: 0.8832\n",
      "19\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0142 - accuracy: 0.9020\n",
      "20\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0149 - accuracy: 0.8852\n",
      "21\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0151 - accuracy: 0.8865\n",
      "22\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0134 - accuracy: 0.9103\n",
      "23\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0148 - accuracy: 0.8898\n",
      "24\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0137 - accuracy: 0.9028\n",
      "25\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0138 - accuracy: 0.8975\n",
      "26\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0129 - accuracy: 0.9128\n",
      "27\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0120 - accuracy: 0.9220\n",
      "28\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0139 - accuracy: 0.9018\n",
      "29\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0127 - accuracy: 0.9103\n",
      "30\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0117 - accuracy: 0.9245\n",
      "31\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0112 - accuracy: 0.9262\n",
      "32\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0116 - accuracy: 0.9222\n",
      "33\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0116 - accuracy: 0.9227\n",
      "34\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0109 - accuracy: 0.9383\n",
      "35\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0106 - accuracy: 0.9320\n",
      "36\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0100 - accuracy: 0.9427\n",
      "37\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0099 - accuracy: 0.9417\n",
      "38\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0096 - accuracy: 0.9413\n",
      "39\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0096 - accuracy: 0.9448\n",
      "40\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0103 - accuracy: 0.9390\n",
      "41\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0105 - accuracy: 0.9348\n",
      "42\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0093 - accuracy: 0.9492\n",
      "43\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0086 - accuracy: 0.9520\n",
      "44\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0083 - accuracy: 0.9565\n",
      "45\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0082 - accuracy: 0.9567\n",
      "46\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0087 - accuracy: 0.9528\n",
      "47\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0084 - accuracy: 0.9520\n",
      "48\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0074 - accuracy: 0.9643\n",
      "49\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0074 - accuracy: 0.9607\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_epoch):\n",
    "\n",
    "    x, y = generate_data(n_samples, allowed_operators, alphabet, word2id, id2word, x_max_length=x_max_length, y_max_length=y_max_length, \n",
    "                    min_value=min_value, max_value=max_value)\n",
    "    print(i)\n",
    "    model.fit(x, y, epochs=1, batch_size=n_batch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0884f4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/training_checkpoints\\\\ckpt_1'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "70773b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  94+49 Expected= 143, Predicted= 143\n",
      "   62+3 Expected=  65, Predicted=  65\n",
      "  80+39 Expected= 119, Predicted= 118\n",
      "  26+83 Expected= 109, Predicted= 109\n",
      "  77+79 Expected= 156, Predicted= 156\n",
      "  88+51 Expected= 139, Predicted= 139\n",
      "  61+33 Expected=  94, Predicted=  94\n",
      "  69+48 Expected= 117, Predicted= 117\n",
      "  35+78 Expected= 113, Predicted= 113\n",
      "  76+73 Expected= 149, Predicted= 149\n",
      "  55+35 Expected=  90, Predicted=  90\n",
      "  56+65 Expected= 121, Predicted= 121\n",
      "  76+11 Expected=  87, Predicted=  87\n",
      "   57+1 Expected=  58, Predicted=  58\n",
      "  54+89 Expected= 143, Predicted= 143\n",
      "   21+5 Expected=  26, Predicted=  27\n",
      "  60+49 Expected= 109, Predicted= 108\n",
      "   99+0 Expected=  99, Predicted=  98\n",
      "  99+79 Expected= 178, Predicted= 178\n",
      "   50+6 Expected=  56, Predicted=  56\n"
     ]
    }
   ],
   "source": [
    "model = build_model(x_max_length, y_max_length, n_chars)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "# evaluate on some new patterns\n",
    "# x, y = generate_data(n_samples, allowed_operators, alphabet, word2id, id2word)    \n",
    "x, y = generate_data(n_samples, allowed_operators, alphabet, word2id, id2word, x_max_length=x_max_length, y_max_length=y_max_length, \n",
    "                    min_value=min_value, max_value=max_value)\n",
    "\n",
    "result = model.predict(x, batch_size=n_batch, verbose=0)\n",
    "\n",
    "# calculate error\n",
    "expected = [invert(x, alphabet) for x in y]\n",
    "predicted = [invert(x, alphabet) for x in result]\n",
    "\n",
    "# show some examples\n",
    "for i in range(20):\n",
    "    print('%s Expected=%s, Predicted=%s' % (invert(x[i], id2word),expected[i], predicted[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
