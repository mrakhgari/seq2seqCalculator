{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a9b6ec",
   "metadata": {},
   "source": [
    "# seq2seqCalculator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aae1df",
   "metadata": {},
   "source": [
    "## Import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8fca7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4444551",
   "metadata": {},
   "source": [
    "## Data\n",
    "One benefit of this task is that you don't need to download any data. You can generate it on your own! The input consists of the following: \"number\"+\"sign\"+\"number\", which then equals \"number\". The pluses stand for concatenation in this case. \n",
    "The seq2seq model doesn't care what symbols we use, we could use \"+-*/\" as symbols. \n",
    "<!-- We use a one-hot encoding for each symbol accroding the assigned number. For numbers this is straight forward and we end up with something like {0:0, 1:1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:7, 8: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83534e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_equations(operators, dataset_size, min_value=0, max_value=10):\n",
    "    '''Generate pairs of equations and solutions to them.\n",
    "        \n",
    "        Each equation has a form of two integers with an operator in between. \n",
    "        Each solution is an integer with the result of the operation.\n",
    "        \n",
    "        \n",
    "        operators: list of strings, allowed operators. \n",
    "        dataset_size: an integer, number of equations to be generated.\n",
    "        min_value: an integer, min value of each operand.\n",
    "        max_value: an integer, max value of each operand.\n",
    "    \n",
    "        result: a list of tuples of strings (equation, solution).\n",
    "    '''\n",
    "    samples = []\n",
    "    number_of_operators = len(operators)\n",
    "    \n",
    "    for _ in range(dataset_size):\n",
    "        equation = (\n",
    "            str(randint(min_value, max_value)) + \n",
    "            operators[randint(0, number_of_operators-1)] +\n",
    "            str(randint(min_value, max_value))\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            solution = str(int(eval(equation)))\n",
    "        except ZeroDivisionError as e: ## handle x/0 state\n",
    "            equation = equation.replace(\"/\", \"*\")\n",
    "            solution = str(int(eval(equation)))\n",
    "\n",
    "        samples.append((equation, solution))\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8a7229d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('5*1', '5')]\n"
     ]
    }
   ],
   "source": [
    "## Test generate_equations\n",
    "test = generate_equations(\"+-/*\",1)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e747f7c1",
   "metadata": {},
   "source": [
    "## Perpare data for the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbfe646",
   "metadata": {},
   "source": [
    "The maximum input length is “length_nr + 1 + length_nr” which is ```x_max_length``` in our case (for example 7). We would like to also have calculations that are shorter. This is entirely possible, however a seq2seq requires [check] inputs of the same length. So when a calculation is smaller then length ```x_max_length``` we fill it up from the left with spaces, e.g. “____2+2”.\n",
    "Also we need to padd the solutions to ```y_max_length```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f7c6d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_to_max(equations, x_max_length, y_max_length):\n",
    "    '''Padd the equations to max lengths.\n",
    "    \n",
    "    equations: a list of tuples of strings (equation, solution)\n",
    "    x_max_length: max_len of equation samples.\n",
    "    y_max_length: max_len of solution samples.\n",
    "    \n",
    "    return:  a list of tuples of strings (equation, solution) that padded to max length.\n",
    "    '''\n",
    "    samples = []\n",
    "    for equation, solution in equations:\n",
    "        samples.append((f'{equation:>{x_max_length}}', f'{solution:>{y_max_length}}'))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0b03aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ids(equations, word2id):\n",
    "    samples = []\n",
    "    for equation, solution in equations:\n",
    "        e = [word2id[c] for c in equation]\n",
    "        s = [word2id[c] for c in solution]\n",
    "        samples.append((e, s))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5bf18a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(equations, labels):\n",
    "    label_binarizer = LabelBinarizer() \n",
    "    label_binarizer.fit(labels)\n",
    "    x, y = [], []\n",
    "    for equation, solution in equations:\n",
    "        x.append(label_binarizer.transform(equation))\n",
    "        y.append(label_binarizer.transform(solution))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "45752b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert encoding\n",
    "def invert(seq, id2word):\n",
    "    strings = list()\n",
    "    for pattern in seq:\n",
    "        string = id2word[np.argmax(pattern)]\n",
    "        strings.append(string)\n",
    "    return ''.join(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0ec03ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_lengths(equations):\n",
    "    x_max_length, y_max_length = 0, 0\n",
    "    for equation, solution in equations:\n",
    "        x_max_length = max(len(equation), x_max_length) \n",
    "        y_max_length = max(len(solution), y_max_length)\n",
    "    return x_max_length, y_max_length\n",
    "\n",
    "\n",
    "def generate_data(n_samples, allowed_operators, alphabet, word2id, id2word):\n",
    "    # generate pairs\n",
    "    data = generate_equations(allowed_operators, n_samples, min_value=0, max_value=10)    \n",
    "    # find max_lengths\n",
    "#     x_max_length, y_max_length = find_max_lengths(data)\n",
    "    \n",
    "    ## padding to max\n",
    "    data = padding_to_max(data, x_max_length, y_max_length)\n",
    "    \n",
    "    ## string to indexs\n",
    "    data = to_ids(data, word2id)\n",
    "    # one hot encoding \n",
    "    x, y = one_hot(data, list(id2word.keys()))\n",
    "\n",
    "    x, y = np.array(x), np.array(y)\n",
    "    \n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c9162281",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_operators = '+-*'\n",
    "n_samples = 1000\n",
    "alphabet = '0123456789' + allowed_operators + ' ' ## space for padding\n",
    "seed(1)\n",
    "\n",
    "word2id = {symbol:i for i, symbol in enumerate(alphabet)}\n",
    "id2word = {i:symbol for symbol, i in word2id.items()}\n",
    "x_max_length, y_max_length = 7, 3\n",
    "x, y = generate_data(n_samples, allowed_operators, alphabet, word2id, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ad51499d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X (1000, 7, 14)\n",
      "shape of y (1000, 3, 14)\n",
      "X[0]:\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "y[0]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "invert X[0]     2*1\n",
      "invert y[0]   2\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of X\", x.shape)\n",
    "print(\"shape of y\", y.shape)\n",
    "print(\"X[0]:\")\n",
    "print(x[0])\n",
    "print(\"y[0]\")\n",
    "print(y[0])\n",
    "print(\"invert X[0]\", invert(x[0], id2word) )\n",
    "print(\"invert y[0]\", invert(y[0], id2word) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c861b6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3f144dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LSTM configuration\n",
    "n_batch = 10\n",
    "n_epoch = 100\n",
    "n_chars = len(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "01f20fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_10 (LSTM)              (None, 100)               46000     \n",
      "                                                                 \n",
      " repeat_vector_5 (RepeatVect  (None, 3, 100)           0         \n",
      " or)                                                             \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 3, 50)             30200     \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 3, 14)            714       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 76,914\n",
      "Trainable params: 76,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(x_max_length, n_chars)))\n",
    "model.add(RepeatVector(y_max_length))\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_chars, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec85fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100/100 [==============================] - 5s 6ms/step - loss: 1.8657 - accuracy: 0.4823\n",
      "1\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.3699 - accuracy: 0.5397\n",
      "2\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.2887 - accuracy: 0.5633\n",
      "3\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.2190 - accuracy: 0.5597\n",
      "4\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.1626 - accuracy: 0.5653\n",
      "5\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.1060 - accuracy: 0.5897\n",
      "6\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.0470 - accuracy: 0.6123\n",
      "7\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 1.0014 - accuracy: 0.6283\n",
      "8\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.9659 - accuracy: 0.6320\n",
      "9\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.9122 - accuracy: 0.6543\n",
      "10\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.8941 - accuracy: 0.6680\n",
      "11\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.8592 - accuracy: 0.6840\n",
      "12\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.8325 - accuracy: 0.6843\n",
      "13\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.8098 - accuracy: 0.7010\n",
      "14\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.7742 - accuracy: 0.7113\n",
      "15\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.7677 - accuracy: 0.7223\n",
      "16\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.7431 - accuracy: 0.7250\n",
      "17\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.7143 - accuracy: 0.7520\n",
      "18\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.6868 - accuracy: 0.7560\n",
      "19\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.6522 - accuracy: 0.7727\n",
      "20\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.5985 - accuracy: 0.8000\n",
      "21\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.5731 - accuracy: 0.8120\n",
      "22\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.5325 - accuracy: 0.8347\n",
      "23\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.4985 - accuracy: 0.8413\n",
      "24\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.4677 - accuracy: 0.8640\n",
      "25\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.4124 - accuracy: 0.8900\n",
      "26\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3818 - accuracy: 0.9060\n",
      "27\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3716 - accuracy: 0.9017\n",
      "28\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3538 - accuracy: 0.9030\n",
      "29\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3041 - accuracy: 0.9390\n",
      "30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.2772 - accuracy: 0.9510\n",
      "31\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.2425 - accuracy: 0.9613\n",
      "32\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.2237 - accuracy: 0.9663\n",
      "33\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.2130 - accuracy: 0.9670\n",
      "34\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1815 - accuracy: 0.9747\n",
      "35\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1859 - accuracy: 0.9697\n",
      "36\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1466 - accuracy: 0.9850\n",
      "37\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1530 - accuracy: 0.9813\n",
      "38\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1412 - accuracy: 0.9810\n",
      "39\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1251 - accuracy: 0.9883\n",
      "40\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1070 - accuracy: 0.9933\n",
      "41\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1070 - accuracy: 0.9913\n",
      "42\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0955 - accuracy: 0.9917\n",
      "43\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0911 - accuracy: 0.9917\n",
      "44\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0905 - accuracy: 0.9907\n",
      "45\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0743 - accuracy: 0.9957\n",
      "46\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0654 - accuracy: 0.9967\n",
      "47\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0679 - accuracy: 0.9957\n",
      "48\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0599 - accuracy: 0.9967\n",
      "49\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0573 - accuracy: 0.9967\n",
      "50\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0524 - accuracy: 0.9960\n",
      "51\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0530 - accuracy: 0.9963\n",
      "52\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0486 - accuracy: 0.9957\n",
      "53\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0399 - accuracy: 0.9987\n",
      "54\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0370 - accuracy: 0.9990\n",
      "55\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0676 - accuracy: 0.9857\n",
      "56\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0497 - accuracy: 0.9937\n",
      "57\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0318 - accuracy: 0.9990\n",
      "58\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0296 - accuracy: 0.9983\n",
      "59\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0235 - accuracy: 1.0000\n",
      "60\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0217 - accuracy: 0.9990\n",
      "61\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0198 - accuracy: 0.9993\n",
      "62\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0204 - accuracy: 0.9993\n",
      "63\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0191 - accuracy: 1.0000\n",
      "64\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0164 - accuracy: 1.0000\n",
      "65\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0170 - accuracy: 0.9997\n",
      "66\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0151 - accuracy: 1.0000\n",
      "67\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0151 - accuracy: 1.0000\n",
      "68\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "69\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0125 - accuracy: 1.0000\n",
      "70\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "71\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "72\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "73\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "74\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "75\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "76\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "77\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0072 - accuracy: 1.0000\n",
      "78\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0068 - accuracy: 1.0000\n",
      "79\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "80\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "81\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "82\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "83\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0050 - accuracy: 1.0000\n",
      "84\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0044 - accuracy: 1.0000\n",
      "85\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "86\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0038 - accuracy: 1.0000\n",
      "87\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "88\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "90\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "91\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "92\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "93\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "94\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "95\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0021 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# train LSTM\n",
    "for i in range(n_epoch):\n",
    "    x, y = generate_data(n_samples, allowed_operators, alphabet, word2id, id2word)    \n",
    "#     X, y = generate_data(n_samples, , largest, alphabet)\n",
    "    print(i)\n",
    "    model.fit(x, y, epochs=1, batch_size=n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "70773b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0+8 Expected=  8, Predicted=  8\n",
      "    1+8 Expected=  9, Predicted=  9\n",
      "    6+8 Expected= 14, Predicted= 14\n",
      "    7-4 Expected=  3, Predicted=  3\n",
      "   10+2 Expected= 12, Predicted= 12\n",
      "    0+2 Expected=  2, Predicted=  2\n",
      "    1+9 Expected= 10, Predicted= 10\n",
      "    3+6 Expected=  9, Predicted=  9\n",
      "    7-7 Expected=  0, Predicted=  0\n",
      "   10-9 Expected=  1, Predicted=  1\n",
      "    8+1 Expected=  9, Predicted=  9\n",
      "    9+6 Expected= 15, Predicted= 15\n",
      "    4+2 Expected=  6, Predicted=  6\n",
      "    5-1 Expected=  4, Predicted=  4\n",
      "    4-7 Expected= -3, Predicted= -3\n",
      "    2+0 Expected=  2, Predicted=  2\n",
      "   10-4 Expected=  6, Predicted=  6\n",
      "    9+1 Expected= 10, Predicted= 10\n",
      "   10+4 Expected= 14, Predicted= 14\n",
      "    9+0 Expected=  9, Predicted=  9\n"
     ]
    }
   ],
   "source": [
    "# evaluate on some new patterns\n",
    "x, y = generate_data(n_samples, allowed_operators, alphabet, word2id, id2word)    \n",
    "\n",
    "result = model.predict(x, batch_size=n_batch, verbose=0)\n",
    "# calculate error\n",
    "expected = [invert(x, alphabet) for x in y]\n",
    "predicted = [invert(x, alphabet) for x in result]\n",
    "# show some examples\n",
    "for i in range(20):\n",
    "    print('%s Expected=%s, Predicted=%s' % (invert(x[i], id2word),expected[i], predicted[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
